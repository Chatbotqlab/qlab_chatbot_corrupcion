{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20efde48",
   "metadata": {},
   "source": [
    "# ## Prueba de API Key de OpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cf352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente OpenAI inicializado correctamente. ¡Tu API key parece funcionar!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Cargar variables de entorno del archivo .env\n",
    "# IMPORTANTE: Asegúrate de tener un archivo llamado '.env' en la raíz de tu proyecto\n",
    "# (al mismo nivel que la carpeta 'procesamiento' o donde ejecutes este notebook).\n",
    "# El contenido del archivo .env debe ser:\n",
    "# OPENAI_API_KEY='AQUÍ_VA_TU_API_KEY_DE_OPENAI'\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"Error: La variable de entorno OPENAI_API_KEY no fue encontrada.\")\n",
    "    print(\"Por favor, crea un archivo .env en la raíz de tu proyecto con tu API key.\")\n",
    "    print(\"Ejemplo de contenido para .env: OPENAI_API_KEY='sk-xxxxxxxxxx'\")\n",
    "else:\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        print(\"Cliente OpenAI inicializado correctamente. ¡Tu API key parece funcionar!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al inicializar el cliente OpenAI: {e}\")\n",
    "        print(\"Verifica que tu API key sea válida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81f6769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enviando pregunta a OpenAI: '¿Cuál es la capital del Perú y un dato interesante sobre ella?'\n",
      "Respuesta de la API: La capital del Perú es Lima. Un dato interesante es que Lima es conocida como la \"Ciudad de los Reyes\" y fue fundada en 1535 por el conquistador Francisco Pizarro.\n"
     ]
    }
   ],
   "source": [
    "if api_key: # Solo ejecutar si la API key se cargó\n",
    "    pregunta_simple = \"¿Cuál es la capital del Perú y un dato interesante sobre ella?\"\n",
    "\n",
    "    # System prompt de prueba (muy simplificado)\n",
    "    # Para tu chatbot completo, usarías el system_prompt_v2 de chatbot_logic.py\n",
    "    system_prompt_prueba = \"Eres un asistente virtual que responde preguntas de forma concisa y amigable.\"\n",
    "\n",
    "    mensajes_para_api = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_prueba},\n",
    "        {\"role\": \"user\", \"content\": pregunta_simple}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nEnviando pregunta a OpenAI: '{pregunta_simple}'\")\n",
    "\n",
    "        # Nota: El modelo \"gpt-4.1-nano\" que usas en chatbot_logic.py no parece ser un modelo estándar de OpenAI.\n",
    "        # Para esta prueba, usaremos \"gpt-3.5-turbo\".\n",
    "        # Si \"gpt-4.1-nano\" es un modelo específico al que tienes acceso (custom model o fine-tuned), puedes cambiarlo.\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",  # Modelo estándar para prueba\n",
    "            messages=mensajes_para_api,\n",
    "            temperature=0.5,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        respuesta = completion.choices[0].message.content\n",
    "        print(f\"Respuesta de la API: {respuesta}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al contactar con OpenAI: {e}\")\n",
    "        print(\"Verifica tu API key, conexión a internet, el modelo especificado y los parámetros de la solicitud.\")\n",
    "else:\n",
    "    print(\"No se puede enviar la pregunta porque la API key no está configurada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676f9248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de chatbot_logic.py importadas correctamente.\n",
      "Archivo .env cargado desde una ubicación por defecto (posiblemente la raíz).\n",
      "Cliente OpenAI inicializado correctamente.\n",
      "Cargados 10620 chunks desde c:\\Users\\alfie\\OneDrive\\Documents\\GitHub\\qlab_chatbot_corrupcion\\output\\salida_chunks_final.jsonl\n",
      "Se cargaron 10620 chunks correctamente.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Importar funciones y variables de chatbot_logic.py\n",
    "# Esta importación asume que este notebook está en la misma carpeta que chatbot_logic.py\n",
    "try:\n",
    "    from chatbot_logic import (\n",
    "        load_chunks_from_jsonl,\n",
    "        send_question_to_openai,\n",
    "        system_prompt_v2,\n",
    "        extract_query_parameters, # Opcional, para pruebas separadas\n",
    "        find_relevant_chunks      # Opcional, para pruebas separadas\n",
    "    )\n",
    "    print(\"Funciones de chatbot_logic.py importadas correctamente.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error al importar desde chatbot_logic.py: {e}\")\n",
    "    print(\"Asegúrate de que este notebook esté en la carpeta 'procesamiento/src_chatbot/' o ajusta el sys.path si es necesario.\")\n",
    "    # Si el notebook está en la raíz del proyecto, la importación sería:\n",
    "    # from procesamiento.src_chatbot.chatbot_logic import (...)\n",
    "\n",
    "# Cargar variables de entorno del archivo .env que debe estar en la raíz del proyecto\n",
    "# (QLAB_CHATBOT_CORRUPCION/.env)\n",
    "try:\n",
    "    # Determinar la ruta raíz del proyecto desde la ubicación actual del notebook\n",
    "    # Asumimos que el notebook está en QLAB_CHATBOT_CORRUPCION/procesamiento/src_chatbot/\n",
    "    current_notebook_dir = os.path.dirname(os.path.abspath(\"__file__\" if \"__file__\" in locals() else os.getcwd()))\n",
    "    project_root_for_env = os.path.abspath(os.path.join(current_notebook_dir, '..', '..'))\n",
    "    dotenv_path = os.path.join(project_root_for_env, '.env')\n",
    "\n",
    "    if os.path.exists(dotenv_path):\n",
    "        load_dotenv(dotenv_path=dotenv_path)\n",
    "        print(f\"Archivo .env cargado desde: {dotenv_path}\")\n",
    "    else:\n",
    "        # Intento de carga alternativo (si el notebook se ejecuta desde la raíz)\n",
    "        if load_dotenv():\n",
    "            print(\"Archivo .env cargado desde una ubicación por defecto (posiblemente la raíz).\")\n",
    "        else:\n",
    "            print(f\"ADVERTENCIA: No se encontró el archivo .env en {dotenv_path} ni en la ubicación por defecto.\")\n",
    "            print(\"Por favor, crea un archivo .env en la raíz del proyecto (QLAB_CHATBOT_CORRUPCION/) con tu OPENAI_API_KEY.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando .env: {e}\")\n",
    "\n",
    "\n",
    "# Configuración del cliente OpenAI\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = None\n",
    "\n",
    "if not api_key:\n",
    "    print(\"ERROR: OPENAI_API_KEY no encontrada en las variables de entorno.\")\n",
    "    print(\"Asegúrate de que el archivo .env esté configurado correctamente en la raíz del proyecto.\")\n",
    "else:\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        print(\"Cliente OpenAI inicializado correctamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al inicializar el cliente OpenAI: {e}\")\n",
    "        client = None # Asegurarse que client es None si falla\n",
    "\n",
    "# Cargar los chunks de documentos\n",
    "# La función load_chunks_from_jsonl en chatbot_logic.py calcula la ruta al archivo.\n",
    "all_docs_chunks = []\n",
    "if 'load_chunks_from_jsonl' in globals(): # Verificar si la importación fue exitosa\n",
    "    try:\n",
    "        all_docs_chunks = load_chunks_from_jsonl() # Esta función ya tiene la ruta al JSONL\n",
    "        if not all_docs_chunks:\n",
    "            print(\"ADVERTENCIA: No se cargaron chunks. Verifica la ruta del archivo 'salida_chunks_final.jsonl' definida en chatbot_logic.py y que el archivo exista y contenga datos.\")\n",
    "        elif len(all_docs_chunks) > 0:\n",
    "            print(f\"Se cargaron {len(all_docs_chunks)} chunks correctamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar chunks: {e}\")\n",
    "        all_docs_chunks = [] # Asegurarse que la lista está vacía si falla\n",
    "else:\n",
    "    print(\"ERROR: La función load_chunks_from_jsonl no pudo ser importada. No se pueden cargar los chunks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8494e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta del usuario: ¿Qué informes de auditoría mencionan irregularidades en Cusco durante el año 2020?\n",
      "\n",
      "Enviando pregunta al chatbot (esto puede tardar unos segundos)...\n",
      "\n",
      "========= RESPUESTA DEL CHATBOT =========\n",
      "Según los informes de auditoría proporcionados, en el año 2020 en la región de Cusco se identificaron algunas irregularidades y observaciones relevantes:\n",
      "\n",
      "1. **Informe 017-2020-2-1627 (Municipalidad Distrital de San Jerónimo):** Este informe tiene como objetivo determinar si la ejecución del contrato para la creación de servicios de salud en la margen derecha del río Huatanay en San Jerónimo, Cusco, se realizó en cumplimiento de la normativa aplicable. Sin embargo, en el fragmento proporcionado no se detallan observaciones específicas sobre irregularidades, solo el objetivo de la auditoría.\n",
      "\n",
      "2. **Informe 048-2020-2-0385 (Municipalidad Provincial del Cusco):** Este informe evaluó el proyecto de inversión para el mejoramiento de la transitabilidad vehicular y peatonal en la calle Asociación de la Urbanización Residencial Huancaro, en Cusco. La auditoría concluyó que se realizó de conformidad con la normativa en sus fases de preinversión e inversión. Sin embargo, también se identificó una observación (Observación n.° 1) relacionada con el deslinde de responsabilidades, y se recomendó disponer el inicio de acciones administrativas para deslindar responsabilidades de los funcionarios y servidores municipales implicados en dicha observación.\n",
      "\n",
      "En resumen, en 2020 en Cusco, el informe 048-2020-2-0385 señala una irregularidad relacionada con la necesidad de deslindar responsabilidades administrativas, mientras que el informe 017-2020-2-1627 no presenta detalles específicos sobre irregularidades en el fragmento consultado.\n",
      "==========================================\n",
      "\n",
      "--- Detalles del procesamiento interno (depuración) ---\n",
      "Parámetros extraídos por extract_query_parameters: {'years': ['2020'], 'regions': ['cusco'], 'keywords': ['informes', 'mencionan', 'irregularidades', 'durante', 'ano'], 'is_specific_enough': True}\n",
      "find_relevant_chunks: Encontrados 3 chunks relevantes (máx. 5 para debug).\n"
     ]
    }
   ],
   "source": [
    "if client and all_docs_chunks:\n",
    "    # --- PREGUNTA DE EJEMPLO ---\n",
    "    # Puedes cambiar esta pregunta para probar diferentes escenarios:\n",
    "    pregunta_usuario = \"¿Qué informes de auditoría mencionan irregularidades en Cusco durante el año 2020?\"\n",
    "    # Otras preguntas para probar:\n",
    "    # pregunta_usuario = \"Resumen de la situación en Ancash para el 2018\"\n",
    "    # pregunta_usuario = \"Háblame del informe 001-2016-2-0335\"\n",
    "    # pregunta_usuario = \"Hay algo sobre Madre de Dios?\" # Para probar 'needs_more_specificity'\n",
    "    # pregunta_usuario = \"Corrupción en Puno 2017\"\n",
    "    # pregunta_usuario = \"Dime qué pasó en Lambayeque en 2019 con obras públicas\"\n",
    "\n",
    "    print(f\"Pregunta del usuario: {pregunta_usuario}\\n\")\n",
    "\n",
    "    # Historial de conversación (vacío para la primera pregunta o puedes simular uno)\n",
    "    conversation_history = [\n",
    "        # Ejemplo de historial:\n",
    "        # {\"role\": \"user\", \"content\": \"Hola\"},\n",
    "        # {\"role\": \"assistant\", \"content\": \"Hola, soy el Chatbot Corrupción. ¿En qué puedo ayudarte?\"}\n",
    "    ]\n",
    "\n",
    "    print(\"Enviando pregunta al chatbot (esto puede tardar unos segundos)...\")\n",
    "    try:\n",
    "        # Llamar a la función principal del chatbot\n",
    "        respuesta_chatbot = send_question_to_openai(\n",
    "            question=pregunta_usuario,\n",
    "            all_docs_chunks=all_docs_chunks,\n",
    "            conversation_history=conversation_history,\n",
    "            openai_client=client, # El cliente OpenAI inicializado\n",
    "            system_prompt=system_prompt_v2  # El system_prompt_v2 importado de chatbot_logic.py\n",
    "        )\n",
    "\n",
    "        print(\"\\n========= RESPUESTA DEL CHATBOT =========\")\n",
    "        print(respuesta_chatbot)\n",
    "        print(\"==========================================\")\n",
    "\n",
    "        # Opcional: Probar la extracción de parámetros y búsqueda de chunks por separado para depuración\n",
    "        print(\"\\n--- Detalles del procesamiento interno (depuración) ---\")\n",
    "        params_debug = extract_query_parameters(pregunta_usuario)\n",
    "        print(f\"Parámetros extraídos por extract_query_parameters: {params_debug}\")\n",
    "\n",
    "        if params_debug.get(\"is_specific_enough\", False):\n",
    "            retrieval_result_debug = find_relevant_chunks(pregunta_usuario, all_docs_chunks, max_chunks=5) # max_chunks bajo para debug\n",
    "            if retrieval_result_debug.get(\"needs_more_specificity\"):\n",
    "                print(\"find_relevant_chunks indica: Necesita más especificidad.\")\n",
    "            elif not retrieval_result_debug.get(\"chunks\"):\n",
    "                print(f\"find_relevant_chunks: No se encontraron chunks. Razón: {retrieval_result_debug.get('no_data_for_filter') or retrieval_result_debug.get('no_data_for_filter_after_score', 'desconocida')}\")\n",
    "                if retrieval_result_debug.get(\"params\"): print(f\"  Parámetros usados por find_relevant_chunks: {retrieval_result_debug.get('params')}\")\n",
    "            else:\n",
    "                print(f\"find_relevant_chunks: Encontrados {len(retrieval_result_debug.get('chunks', []))} chunks relevantes (máx. 5 para debug).\")\n",
    "                # Puedes descomentar lo siguiente para ver detalles de los chunks recuperados:\n",
    "                # for i, chunk_info in enumerate(retrieval_result_debug.get('chunks', [])):\n",
    "                #     print(f\"  Chunk {i+1}: Informe {chunk_info['metadata'].get('numero_informe', 'N/A')}, Tipo: {chunk_info.get('source_field', 'N/A')}, Score (si aplica): {chunk_info.get('score_debug', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"La pregunta no fue lo suficientemente específica según extract_query_parameters.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al procesar la pregunta con send_question_to_openai: {e}\")\n",
    "        print(\"Verifica tu API key, el modelo 'gpt-4.1-nano' (si es un modelo al que tienes acceso o si necesitas cambiarlo a uno estándar como 'gpt-3.5-turbo'), y tu conexión a internet.\")\n",
    "\n",
    "elif not client:\n",
    "    print(\"El cliente de OpenAI no está inicializado. Verifica tu API key en el archivo .env.\")\n",
    "elif not all_docs_chunks:\n",
    "    print(\"No hay chunks cargados. No se puede procesar la pregunta. Verifica que 'salida_chunks_final.jsonl' exista y sea accesible.\")\n",
    "else:\n",
    "    print(\"Error desconocido en la configuración inicial.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
