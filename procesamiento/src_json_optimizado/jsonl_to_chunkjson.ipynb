{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bea70444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9847e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta del archivo consolidado de entrada: c:\\Users\\LENOVO\\Documents\\GitHub\\qlab_chatbot_corrupcion\\output\\salida_informes_consolidados_desde_csv.jsonl\n",
      "Ruta del archivo de chunks de salida: c:\\Users\\LENOVO\\Documents\\GitHub\\qlab_chatbot_corrupcion\\output\\salida_chunks_final.jsonl\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque para definir rutas dinámicamente ---\n",
    "\n",
    "# El notebook está en: .../QLAB_CHATBOT_CORRUPCION/procesamiento/src_json_optimizado/\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Subimos dos niveles para llegar a la raíz del proyecto: QLAB_CHATBOT_CORRUPCION/\n",
    "# 1. de src_json_optimizado a procesamiento ('..')\n",
    "# 2. de procesamiento a QLAB_CHATBOT_CORRUPCION ('..')\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..'))\n",
    "\n",
    "# Nombres de los archivos\n",
    "consolidated_filename = \"salida_informes_consolidados_desde_csv.jsonl\" # Tu archivo de entrada\n",
    "chunked_filename = \"salida_chunks_final.jsonl\" # Tu archivo de salida\n",
    "\n",
    "# Construir rutas completas a los archivos en la carpeta 'output' del proyecto\n",
    "consolidated_file_path = os.path.join(project_root, 'output', consolidated_filename)\n",
    "chunked_output_file_path = os.path.join(project_root, 'output', chunked_filename)\n",
    "\n",
    "# (Opcional) Imprime las rutas para verificar:\n",
    "print(f\"Ruta del archivo consolidado de entrada: {consolidated_file_path}\")\n",
    "print(f\"Ruta del archivo de chunks de salida: {chunked_output_file_path}\")\n",
    "# --- Fin del bloque de rutas ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfbdae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_text_block_into_items(text_block, item_prefix_regex_str=None, default_if_no_prefix=True):\n",
    "    \"\"\"\n",
    "    Divide un bloque de texto en ítems.\n",
    "    Si item_prefix_regex_str es None o no encuentra matches, y default_if_no_prefix es True,\n",
    "    trata todo el bloque como un solo ítem.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    if not text_block or pd.isna(text_block): # Manejar None o NaN explícitamente\n",
    "        return items\n",
    "        \n",
    "    text_block = str(text_block).strip()\n",
    "    if not text_block: # Si después de strip es vacío\n",
    "        return items\n",
    "\n",
    "    if item_prefix_regex_str:\n",
    "        # Encontrar todos los inicios de ítems y sus textos\n",
    "        matches = list(re.finditer(item_prefix_regex_str, text_block, re.MULTILINE))\n",
    "        \n",
    "        if matches:\n",
    "            for i, match_obj in enumerate(matches):\n",
    "                # El texto del ítem es desde el final del prefijo actual\n",
    "                # hasta el inicio del prefijo del siguiente ítem (o el final del bloque)\n",
    "                text_start_index = match_obj.end() # Comienza después del prefijo\n",
    "                \n",
    "                text_end_index = matches[i+1].start() if i + 1 < len(matches) else len(text_block)\n",
    "                \n",
    "                item_text = text_block[text_start_index:text_end_index].replace(\"\\n\", \" \").strip()\n",
    "                if item_text: # Solo añadir si no está vacío\n",
    "                    items.append(item_text)\n",
    "            return items\n",
    "\n",
    "    # Si no hay prefijo o no se encontraron matches con el prefijo, y default_if_no_prefix es True\n",
    "    if default_if_no_prefix and text_block:\n",
    "        return [text_block.replace(\"\\n\", \" \").strip()]\n",
    "    \n",
    "    return items\n",
    "\n",
    "\n",
    "def chunk_consolidated_reports(consolidated_jsonl_path, chunked_jsonl_path):\n",
    "    \"\"\"\n",
    "    Lee un JSONL de informes consolidados y genera un nuevo JSONL con chunks individuales\n",
    "    para objetivos, observaciones y recomendaciones.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Patrones regex para identificar ítems dentro de los bloques de texto\n",
    "    # (ajustar según la estructura real de tus bloques de texto)\n",
    "    # Para objetivos específicos que empiezan con guion:\n",
    "    obj_especifico_pattern = r\"^\\s*-\\s+\"\n",
    "    # Para observaciones/recomendaciones numeradas:\n",
    "    obs_rec_pattern = r\"^\\s*\\d+\\s*[\\.-]\\s*\"\n",
    "\n",
    "\n",
    "    with open(consolidated_jsonl_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(chunked_jsonl_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "        for line in infile:\n",
    "            try:\n",
    "                report_data = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decodificando JSON en la línea: {line.strip()}. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            report_id = report_data.get(\"report_id\", \"ID_DESCONOCIDO\")\n",
    "            base_metadata = {\n",
    "                \"numero_informe\": report_data.get(\"numero_informe\"),\n",
    "                \"titulo_informe\": report_data.get(\"titulo_informe\"),\n",
    "                \"region\": report_data.get(\"region\"),\n",
    "                \"provincia\": report_data.get(\"provincia\"),\n",
    "                \"distrito\": report_data.get(\"distrito\"),\n",
    "                \"entidad_auditada\": report_data.get(\"entidad_auditada\"),\n",
    "                \"year\": report_data.get(\"year\"),\n",
    "                \"periodo_inicio\": report_data.get(\"periodo_inicio\"),\n",
    "                \"periodo_fin\": report_data.get(\"periodo_fin\"),\n",
    "                \"monto_auditado\": report_data.get(\"monto_auditado\"),\n",
    "                \"monto_examinado\": report_data.get(\"monto_examinado\"),\n",
    "                \"modalidad\": report_data.get(\"modalidad\"),\n",
    "                \"fecha_emision\": report_data.get(\"fecha_emision\"),\n",
    "                \"unidad_emite\": report_data.get(\"unidad_emite\"),\n",
    "                \"personas_implicadas\": report_data.get(\"personas_implicadas_consolidado\", []),\n",
    "                # Incluir responsabilidades consolidadas\n",
    "                **report_data.get(\"responsabilidades_consolidadas\", {}) # Desempaqueta el dict de responsabilidades\n",
    "            }\n",
    "            \n",
    "            chunk_idx_counter = {\"obj\": 0, \"obs\": 0, \"rec\": 0}\n",
    "\n",
    "            # 1. Procesar Objetivos\n",
    "            objetivos_block = report_data.get(\"texto_objetivos_completo\", \"\")\n",
    "            if objetivos_block:\n",
    "                # Intentar separar objetivo general de específicos\n",
    "                # Esta es una lógica simple, puede necesitar ajustes robustos\n",
    "                general_obj_text = \"\"\n",
    "                especificos_block_text = objetivos_block # Por defecto, todo es específico si no se encuentra \"general\"\n",
    "\n",
    "                # Patrones para encontrar \"Objetivo General\" y \"Objetivos Específicos\"\n",
    "                # Se hacen no sensibles a mayúsculas/minúsculas con re.IGNORECASE\n",
    "                match_general = re.search(r\"(Objetivo\\s+General[:\\s\\n]+|2\\.1\\s*Objetivo\\s*general\\s*\\n)([\\s\\S]+?)(?=(Objetivos\\s+Específicos[:\\s\\n]+|2\\.2\\s*Objetivos\\s*específicos|\\Z))\", objetivos_block, re.IGNORECASE | re.DOTALL)\n",
    "                \n",
    "                if match_general:\n",
    "                    general_obj_text = match_general.group(2).replace(\"\\n\", \" \").strip()\n",
    "                    # El resto sería para específicos, si \"Objetivos Específicos\" sigue\n",
    "                    idx_fin_general = match_general.end()\n",
    "                    match_especificos_header = re.search(r\"(Objetivos\\s+Específicos[:\\s\\n]+|2\\.2\\s*Objetivos\\s*específicos\\s*\\n)\", objetivos_block[idx_fin_general:], re.IGNORECASE)\n",
    "                    if match_especificos_header:\n",
    "                         especificos_block_text = objetivos_block[idx_fin_general + match_especificos_header.end():]\n",
    "                    else: # No hay encabezado de específicos, o el general fue el último\n",
    "                         especificos_block_text = \"\" \n",
    "                \n",
    "                if general_obj_text:\n",
    "                    chunk_id = f\"{report_id}_obj_{chunk_idx_counter['obj']}\"\n",
    "                    chunk_data = {\n",
    "                        \"chunk_id\": chunk_id, \"report_id\": report_id,\n",
    "                        \"chunk_text\": general_obj_text, \"source_field\": \"objetivo\",\n",
    "                        \"metadata\": base_metadata.copy()\n",
    "                    }\n",
    "                    outfile.write(json.dumps(chunk_data, ensure_ascii=False) + '\\n')\n",
    "                    chunk_idx_counter['obj'] += 1\n",
    "                \n",
    "                # Procesar objetivos específicos\n",
    "                if especificos_block_text:\n",
    "                    lista_obj_especificos = parse_text_block_into_items(especificos_block_text, obj_especifico_pattern)\n",
    "                    for obj_esp_text in lista_obj_especificos:\n",
    "                        if not obj_esp_text: continue\n",
    "                        chunk_id = f\"{report_id}_obj_{chunk_idx_counter['obj']}\"\n",
    "                        chunk_data = {\n",
    "                            \"chunk_id\": chunk_id, \"report_id\": report_id,\n",
    "                            \"chunk_text\": obj_esp_text, \"source_field\": \"objetivo\",\n",
    "                            \"metadata\": base_metadata.copy()\n",
    "                        }\n",
    "                        outfile.write(json.dumps(chunk_data, ensure_ascii=False) + '\\n')\n",
    "                        chunk_idx_counter['obj'] += 1\n",
    "                elif not general_obj_text and objetivos_block: # Si no se separó general y hay texto, tratarlo como un solo objetivo\n",
    "                    chunk_id = f\"{report_id}_obj_{chunk_idx_counter['obj']}\"\n",
    "                    chunk_data = {\n",
    "                        \"chunk_id\": chunk_id, \"report_id\": report_id,\n",
    "                        \"chunk_text\": objetivos_block.replace(\"\\n\", \" \").strip(), \"source_field\": \"objetivo\",\n",
    "                        \"metadata\": base_metadata.copy()\n",
    "                    }\n",
    "                    outfile.write(json.dumps(chunk_data, ensure_ascii=False) + '\\n')\n",
    "                    chunk_idx_counter['obj'] += 1\n",
    "\n",
    "\n",
    "            # 2. Procesar Observaciones\n",
    "            observaciones_block = report_data.get(\"texto_observaciones_completo\", \"\")\n",
    "            if observaciones_block:\n",
    "                lista_observaciones = parse_text_block_into_items(observaciones_block, obs_rec_pattern)\n",
    "                for obs_text in lista_observaciones:\n",
    "                    if not obs_text: continue\n",
    "                    chunk_id = f\"{report_id}_obs_{chunk_idx_counter['obs']}\"\n",
    "                    chunk_data = {\n",
    "                        \"chunk_id\": chunk_id, \"report_id\": report_id,\n",
    "                        \"chunk_text\": obs_text, \"source_field\": \"observacion\",\n",
    "                        \"metadata\": base_metadata.copy()\n",
    "                    }\n",
    "                    outfile.write(json.dumps(chunk_data, ensure_ascii=False) + '\\n')\n",
    "                    chunk_idx_counter['obs'] += 1\n",
    "\n",
    "            # 3. Procesar Recomendaciones\n",
    "            recomendaciones_block = report_data.get(\"texto_recomendaciones_completo\", \"\")\n",
    "            if recomendaciones_block:\n",
    "                # La recomendación puede tener un destinatario antes de la numeración.\n",
    "                # Intentaremos quitarlo si existe \"Al [Cargo/Entidad]:\" o similar antes de pasar a parse_text_block_into_items\n",
    "                # O ajustar parse_text_block_into_items para manejarlo.\n",
    "                # Por ahora, se asume que el patrón numérico es lo principal.\n",
    "                lista_recomendaciones = parse_text_block_into_items(recomendaciones_block, obs_rec_pattern)\n",
    "                for rec_text in lista_recomendaciones:\n",
    "                    if not rec_text: continue\n",
    "                    # Quitar posible destinatario al inicio si el patrón es \"Al ... N. texto\"\n",
    "                    # Esto es opcional y depende de cómo quieras el chunk_text final\n",
    "                    # rec_text_cleaned = re.sub(r\"^(Al\\s+.*?[\\n:])?\\s*\\d+\\s*[\\.-]\\s*\", \"\", rec_text, flags=re.IGNORECASE).strip()\n",
    "                    # if not rec_text_cleaned: rec_text_cleaned = rec_text # Si la limpieza lo dejó vacío, usar el original\n",
    "\n",
    "                    chunk_id = f\"{report_id}_rec_{chunk_idx_counter['rec']}\"\n",
    "                    chunk_data = {\n",
    "                        \"chunk_id\": chunk_id, \"report_id\": report_id,\n",
    "                        \"chunk_text\": rec_text, # Usar rec_text o rec_text_cleaned\n",
    "                        \"source_field\": \"recomendacion\",\n",
    "                        \"metadata\": base_metadata.copy()\n",
    "                    }\n",
    "                    outfile.write(json.dumps(chunk_data, ensure_ascii=False) + '\\n')\n",
    "                    chunk_idx_counter['rec'] += 1\n",
    "                    \n",
    "    print(f\"Archivo JSONL con chunks individuales '{chunked_jsonl_path}' generado exitosamente.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c154ee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo JSONL con chunks individuales 'c:\\Users\\LENOVO\\Documents\\GitHub\\qlab_chatbot_corrupcion\\output\\salida_chunks_final.jsonl' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Llamada a la función principal\n",
    "chunk_consolidated_reports(consolidated_file_path, chunked_output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
